---
title: "Improving Language Understanding for Low-Resource Languages and Tasks with Generative Pre-Training"
subtitle: "Deep Learning Camp Jeju 2018"  
author: "Ali Zaidi"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
mono_accent_inverse(
  base_color = "#5B88A6",
  code_font_family = "Fira Code",
  inverse_header_color = "#000000",
  code_font_url    = "https://cdn.rawgit.com/tonsky/FiraCode/1.204/distr/fira_code.css"
)
```

## Capacity and Inductive Biases
### Learning Curves: Learning from Experience

![](imgs/learning-curves.png)

---

## Pre-Training and Knowledge Sharing
### Success in Vision from Pre-Trained Models

--

<img src="imgs/distill-vis.png" height="250">

--

* In CV, combination of ImageNet and CNNs has been a big success
    - enabled highly performant models
    - better understanding of the models
    - more inspired from how perception/vision may work
* NLP:
    - still stuck on word embeddings
    - single-task models

---

background-image: url("https://thumbs.gfycat.com/OrangeSinfulAcornbarnacle-max-14mb.gif")
background-size: cover

>You can't cram the meaning of a whole %&!$ing sentence into a single $&!*ing vector!

Ray Mooney

---

## Data Scarcity, Non-generalizeable Features


Dataset | Domain | Size (#sentences)
---------|----------|---------
 CoNLL 2003 | NER | 15K
 OntoNotes | Coreference Resolution | 75K
 PTB | Parsing | 40K
 SQuAD | QA | 100K
 SNLI | Entailment | 570K
 SST | Sentiment | 10K

--

- Most SOTA NLP results are obtained by training end-to-end architectures for each language task. 
- Most datasets are in English, and are very small.
- Not much knowledge sharing, or re-use
- Single task learning inhibits us from general language understanding, and exposes us to learning overly discriminative features that generalize poorly


---

## Representations of Language
### Language Modeling

* **Goal**:  predict the next word given the preceding words

* Given a sequence of words $x^{(1)}, x^{(2)},\ldots,x^{(t)}$, compute the CPD:

$$
\mathbb{P}\left(x^{(t+1)}=w_j \vert x^{(t)},\ldots,x^{(1)}\right)
$$

---

## Literature Review

### Fine-Tuning Language Models

1. [OpenAI: Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/)
    * **_tldr_**: Train an unsupervised language model using a transformer architecture, and then fine-tune on task-specific datasets.
2. [fastAI: Universal Language Model Fine-tuning for Text Classification](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)
    * **_tldr_**: Pre-train a language model on large unlabelled corpus. Initialize new language model on your unlabeled domain-specific corpus. Fine-tune task-domain-specific architecture for text classification.
    ![](imgs/ulmfit.png)

---

## Generative Language Modeling Helps

### Common Sense Reasoning

3. [Trieu H. Trinh & Quoc Le: A Simple Method for Commonsense Reasoning](https://arxiv.org/abs/1806.02847)
    - **__tldr__**: Solve multiple choice questions from _Pronoun Disambiguation Challenge_ and _Winograd Schema Challenge_ by pre-training many language models (diversity helps!), substitute question in sentence, pick one with highest likelihood on the LMs (using ensembling).
    <img src="imgs/commonsense.png" height="250">
    

---

## Unsupervised Neural Machine Translation
## Adversarial Alignment of Embedding and Sentence Encoders 

--

![](imgs/muse-alignment.png)

--

$$\mathcal{L}_{\mathcal{D}}\left(\theta_{\mathcal{D}}\vert\boldsymbol{W}\right)=-\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta_{\mathcal{D}}}\left({\rm source}=1\vert W_{x_{i}}\right)-\frac{1}{m}\sum_{i=1}^{m}\log P_{\theta_{\mathcal{D}}}\left({\rm source}=0\vert y_{i}\right)$$
* complete implementation: [`src/scripts/opennmt-unmt`](https://github.com/akzaidi/fine-lm/tree/master/src/scripts/opennmt-unmt)
* instructions: [`github.com/akzaidi/fine-lm#neural-machine-translation-baselines`](https://github.com/akzaidi/fine-lm#neural-machine-translation-baselines)

---

## Unsupervised Neural Machine Translation

### Shared English-Russian Embeddings

<img src="imgs/en-ruski-muse.png" height="450">


---



---

class: center
background-image: url(https://cdn-images-1.medium.com/max/2000/1*IfjQyGlgAIo8yCvZwtk4CA.jpeg)
background-size: cover


# Thanks!

<p><font size="4" color="black">Thanks to TF Korea, the Oh S crew, all the wonderful organizers and mentors ♥Bitnoori, SangJin, Terry, Soonson, Wonchang, Eric!!</font></p>

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

<p><font size="5" color="white">Thanks to my mentor Minjoon Seo!</font></p>

<p><font face="monospace" color="white">github.com/akzaidi/fine-lm</font></p>



---


class: center
background-image: url(imgs/dljejupz.gif)
background-size: cover

<p><font size="7" color="black">고맙습니다. 다음에 봐! </font></p>
<p><font size="7" color="red">잘 잤어 &#x1f48b; </font></p>