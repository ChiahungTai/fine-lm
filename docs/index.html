<!DOCTYPE html>
<html>
  <head>
    <title>Improving Language Understanding for Low-Resource Languages and Tasks with Generative Pre-Training</title>
    <meta charset="utf-8">
    <meta name="author" content="Ali Zaidi" />
    <meta name="date" content="2018-07-26" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Improving Language Understanding for Low-Resource Languages and Tasks with Generative Pre-Training
## Deep Learning Camp Jeju 2018
### Ali Zaidi
### 2018-07-26

---






## Capacity and Inductive Biases
### Learning Curves: Learning from Experience

![](imgs/learning-curves.png)

---

## Pre-Training and Knowledge Sharing
### Success in Vision from Pre-Trained Models

--

&lt;img src="imgs/distill-vis.png" height="250"&gt;

--

* In CV, combination of ImageNet and CNNs has been a big success
    - enabled highly performant models
    - better understanding of the models
    - more inspired from how perception/vision may work
* NLP:
    - still stuck on word embeddings
    - single-task models

---

background-image: url("https://thumbs.gfycat.com/OrangeSinfulAcornbarnacle-max-14mb.gif")
background-size: cover

&gt;You can't cram the meaning of a whole %&amp;!$ing sentence into a single $&amp;!*ing vector!

Ray Mooney

---

## Data Scarcity, Non-generalizeable Features


Dataset | Domain | Size (#sentences)
---------|----------|---------
 CoNLL 2003 | NER | 15K
 OntoNotes | Coreference Resolution | 75K
 PTB | Parsing | 40K
 SQuAD | QA | 100K
 SNLI | Entailment | 570K
 SST | Sentiment | 10K

--

- Most SOTA NLP results are obtained by training end-to-end architectures for each language task. 
- Most datasets are in English, and are very small.
- Not much knowledge sharing, or re-use
- Single task learning inhibits us from general language understanding, and exposes us to learning overly discriminative features that generalize poorly


---

## Representations of Language
### Language Modeling

* **Goal**:  predict the next word given the preceding words

* Given a sequence of words `\(x^{(1)}, x^{(2)},\ldots,x^{(t)}\)`, compute the CPD:

$$
\mathbb{P}\left(x^{(t+1)}=w_j \vert x^{(t)},\ldots,x^{(1)}\right)
$$

---

## Literature Review

### Fine-Tuning Language Models

1. [OpenAI: Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/)
    * **_tldr_**: Train an unsupervised language model using a transformer architecture, and then fine-tune on task-specific datasets.
2. [fastAI: Universal Language Model Fine-tuning for Text Classification](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)
    * **_tldr_**: Pre-train a language model on large unlabelled corpus. Initialize new language model on your unlabeled domain-specific corpus. Fine-tune task-domain-specific architecture for text classification.
    ![](imgs/ulmfit.png)

---

## Generative Language Modeling Helps

### Common Sense Reasoning

3. [Trieu H. Trinh &amp; Quoc Le: A Simple Method for Commonsense Reasoning](https://arxiv.org/abs/1806.02847)
    - **__tldr__**: Solve multiple choice questions from _Pronoun Disambiguation Challenge_ and _Winograd Schema Challenge_ by pre-training many language models (diversity helps!), substitute question in sentence, pick one with highest likelihood on the LMs (using ensembling).
    &lt;img src="imgs/commonsense.png" height="250"&gt;
    

---

## Unsupervised Neural Machine Translation
## Adversarial Alignment of Embedding and Sentence Encoders 

--

![](imgs/muse-alignment.png)

--

`$$\mathcal{L}_{\mathcal{D}}\left(\theta_{\mathcal{D}}\vert\boldsymbol{W}\right)=-\frac{1}{n}\sum_{i=1}^{n}\log P_{\theta_{\mathcal{D}}}\left({\rm source}=1\vert W_{x_{i}}\right)-\frac{1}{m}\sum_{i=1}^{m}\log P_{\theta_{\mathcal{D}}}\left({\rm source}=0\vert y_{i}\right)$$`
* complete implementation: [`src/scripts/opennmt-unmt`](https://github.com/akzaidi/fine-lm/tree/master/src/scripts/opennmt-unmt)
* instructions: [`github.com/akzaidi/fine-lm#neural-machine-translation-baselines`](https://github.com/akzaidi/fine-lm#neural-machine-translation-baselines)

---

## Unsupervised Neural Machine Translation

### Shared English-Russian Embeddings

&lt;img src="imgs/en-ruski-muse.png" height="450"&gt;


---

## Transformer Language Models
### TPU Training with `Tensor2Tensor`

.pull-left[

- Bidirectional LSTMs have become the workhorse for encoder-decoder Seq2Seq Models
- Good:
    * perhaps the proper inductive bias: iterative/recursive transformations
- Some limitations: 
    * inherently sequential (hard to parallelize)
    * forgetting ("LSTMs work in practice, but can they work in theory?")
]

--

.pull-right[

- Transformers:
    * Relies solely on self-attention to learn long dependencies 

&lt;img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png" height="250"&gt;


]

---

## Transformer Training on TPUs


* Implementation in `tensor2tensor`:
* Define `data_generator` for source data to `tensor2tensor/data_generator`
    - decorate with `@registry.register_problem`
    - annotate necessary methods, i.e., `approx_vocab`
* Add it to `all_problems.py` list in `tensor2tensor` library
* Language model on wikitext-103: took ~12 hours on TPU to attain perplexity of 53.2, very close to the [SOTA reported perplexity](https://arxiv.org/pdf/1708.02182.pdf)



---

class: center
background-image: url(https://cdn-images-1.medium.com/max/2000/1*IfjQyGlgAIo8yCvZwtk4CA.jpeg)
background-size: cover


# Thanks!

&lt;p&gt;&lt;font size="4" color="black"&gt;Thanks to TF Korea, the Oh S crew, all the wonderful organizers and mentors ♥Bitnoori, SangJin, Terry, Soonson, Wonchang, Eric!!&lt;/font&gt;&lt;/p&gt;

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

&lt;p&gt;&lt;font size="5" color="white"&gt;Thanks to my mentor Minjoon Seo!&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;font face="monospace" color="white"&gt;github.com/akzaidi/fine-lm&lt;/font&gt;&lt;/p&gt;



---


class: center
background-image: url(imgs/dljejupz.gif)
background-size: cover

&lt;p&gt;&lt;font size="7" color="black"&gt;고맙습니다. 다음에 봐! &lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size="7" color="red"&gt;잘 잤어 &amp;#x1f48b; &lt;/font&gt;&lt;/p&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
